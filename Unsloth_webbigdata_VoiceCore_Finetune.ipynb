{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/webbigdata-jp/VoiceCore/blob/main/Unsloth_webbigdata_VoiceCore_Finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25pkuDXVYVOx"
      },
      "source": [
        "これはUnslothを使ってVoiceCoreをFinetuneするサンプルコードです。  \n",
        "VoiceCoreは英語モデルであるorpheusを日本語で継続事前学習したモデルですがその過程で英語は流暢に話せなくなっています。  \n",
        "これをFinetuneで改良しましょう\n",
        "\n",
        "This is sample code to finetune VoiceCore with Unsloth.  \n",
        "VoiceCore is a model that has been continuously pre-trained in Japanese on the English model Orpheus, but in the process, it has lost the ability to speak English fluently.\n",
        "Let's improve this with Finetune.\n",
        "\n",
        "元のスクリプトとデータを提供してくれた[Unsloth](https://huggingface.co/unsloth)、[Etherl](https://huggingface.co/Etherll)、[MrDragonFox](https://huggingface.co/datasets/MrDragonFox/Elise)に感謝します。  \n",
        "Thank you to [Unsloth](https://huggingface.co/unsloth) and [Etherl](https://huggingface.co/Etherll), [MrDragonFox](https://huggingface.co/datasets/MrDragonFox/Elise) for creating original notebook and data!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OzZjg_-YVOy"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUvTDJGbYVOz"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install snac"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkWYsztAs9Ky"
      },
      "source": [
        "### Unsloth finetune\n",
        "\n",
        "Original note book is [here](https://unsloth.ai/blog/tts)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-22T00:48:54.511089Z",
          "iopub.status.busy": "2025-03-22T00:48:54.510770Z",
          "iopub.status.idle": "2025-03-22T00:51:37.363415Z",
          "shell.execute_reply": "2025-03-22T00:51:37.362696Z",
          "shell.execute_reply.started": "2025-03-22T00:48:54.511053Z"
        },
        "id": "QmUBVEnvCDJv",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"webbigdata/VoiceCore\",\n",
        "    max_seq_length= 2048, # Choose any for long context!\n",
        "    dtype = None, # Select None for auto detection\n",
        "    load_in_4bit = False, # Select True for 4bit which reduces memory usage\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's check VoiceCore English Speking ability.\n",
        "\n",
        "まず、オリジナルモデルの英語能力を確認しましょう  \n",
        "First, let's check the English ability of the original model.  "
      ],
      "metadata": {
        "id": "CPG__7fTueaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [\n",
        "    \"Hey there my name is Elise, <giggles> and I'm a speech generation model that can sound like a person.\",\n",
        "]\n",
        "\n",
        "chosen_voice = \"Elisa\" # None for single-speaker"
      ],
      "metadata": {
        "id": "rpZZT3STvrPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "# Moving snac_model cuda to cpu\n",
        "from snac import SNAC\n",
        "snac_model = SNAC.from_pretrained(\"hubertsiuzdak/snac_24khz\")\n",
        "snac_model.to(\"cpu\")\n",
        "\n",
        "prompts_ = [(f\"{chosen_voice}: \" + p) if chosen_voice else p for p in prompts]\n",
        "\n",
        "def speak_voice():\n",
        "  all_input_ids = []\n",
        "\n",
        "  for prompt in prompts_:\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "    all_input_ids.append(input_ids)\n",
        "\n",
        "  start_token = torch.tensor([[ 128259]], dtype=torch.int64) # Start of human\n",
        "  end_tokens = torch.tensor([[128009, 128260]], dtype=torch.int64) # End of text, End of human\n",
        "\n",
        "  all_modified_input_ids = []\n",
        "  for input_ids in all_input_ids:\n",
        "    modified_input_ids = torch.cat([start_token, input_ids, end_tokens], dim=1) # SOH SOT Text EOT EOH\n",
        "    all_modified_input_ids.append(modified_input_ids)\n",
        "\n",
        "  all_padded_tensors = []\n",
        "  all_attention_masks = []\n",
        "  max_length = max([modified_input_ids.shape[1] for modified_input_ids in all_modified_input_ids])\n",
        "  for modified_input_ids in all_modified_input_ids:\n",
        "    padding = max_length - modified_input_ids.shape[1]\n",
        "    padded_tensor = torch.cat([torch.full((1, padding), 128263, dtype=torch.int64), modified_input_ids], dim=1)\n",
        "    attention_mask = torch.cat([torch.zeros((1, padding), dtype=torch.int64), torch.ones((1, modified_input_ids.shape[1]), dtype=torch.int64)], dim=1)\n",
        "    all_padded_tensors.append(padded_tensor)\n",
        "    all_attention_masks.append(attention_mask)\n",
        "\n",
        "  all_padded_tensors = torch.cat(all_padded_tensors, dim=0)\n",
        "  all_attention_masks = torch.cat(all_attention_masks, dim=0)\n",
        "\n",
        "  input_ids = all_padded_tensors.to(\"cuda\")\n",
        "  attention_mask = all_attention_masks.to(\"cuda\")\n",
        "  generated_ids = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_new_tokens=1200,\n",
        "        do_sample=True,\n",
        "        temperature=0.6,\n",
        "        top_p=0.95,\n",
        "        repetition_penalty=1.1,\n",
        "        num_return_sequences=1,\n",
        "        eos_token_id=128258,\n",
        "      use_cache = True\n",
        "    )\n",
        "  token_to_find = 128257\n",
        "  token_to_remove = 128258\n",
        "\n",
        "  token_indices = (generated_ids == token_to_find).nonzero(as_tuple=True)\n",
        "\n",
        "  if len(token_indices[1]) > 0:\n",
        "      last_occurrence_idx = token_indices[1][-1].item()\n",
        "      cropped_tensor = generated_ids[:, last_occurrence_idx+1:]\n",
        "  else:\n",
        "      cropped_tensor = generated_ids\n",
        "\n",
        "  mask = cropped_tensor != token_to_remove\n",
        "\n",
        "  processed_rows = []\n",
        "\n",
        "  for row in cropped_tensor:\n",
        "      masked_row = row[row != token_to_remove]\n",
        "      processed_rows.append(masked_row)\n",
        "\n",
        "  code_lists = []\n",
        "\n",
        "  for row in processed_rows:\n",
        "      row_length = row.size(0)\n",
        "      new_length = (row_length // 7) * 7\n",
        "      trimmed_row = row[:new_length]\n",
        "      trimmed_row = [t - 128266 for t in trimmed_row]\n",
        "      code_lists.append(trimmed_row)\n",
        "\n",
        "\n",
        "  def redistribute_codes(code_list):\n",
        "    layer_1 = []\n",
        "    layer_2 = []\n",
        "    layer_3 = []\n",
        "    for i in range((len(code_list)+1)//7):\n",
        "      layer_1.append(code_list[7*i])\n",
        "      layer_2.append(code_list[7*i+1]-4096)\n",
        "      layer_3.append(code_list[7*i+2]-(2*4096))\n",
        "      layer_3.append(code_list[7*i+3]-(3*4096))\n",
        "      layer_2.append(code_list[7*i+4]-(4*4096))\n",
        "      layer_3.append(code_list[7*i+5]-(5*4096))\n",
        "      layer_3.append(code_list[7*i+6]-(6*4096))\n",
        "    codes = [torch.tensor(layer_1).unsqueeze(0),\n",
        "          torch.tensor(layer_2).unsqueeze(0),\n",
        "          torch.tensor(layer_3).unsqueeze(0)]\n",
        "\n",
        "    # codes = [c.to(\"cuda\") for c in codes]\n",
        "    audio_hat = snac_model.decode(codes)\n",
        "    return audio_hat\n",
        "\n",
        "  my_samples = []\n",
        "  for code_list in code_lists:\n",
        "    samples = redistribute_codes(code_list)\n",
        "    my_samples.append(samples)\n",
        "  from IPython.display import display, Audio\n",
        "  if len(prompts) != len(my_samples):\n",
        "    raise Exception(\"Number of prompts and samples do not match\")\n",
        "  else:\n",
        "    for i in range(len(my_samples)):\n",
        "      print(prompts[i])\n",
        "      samples = my_samples[i]\n",
        "      display(Audio(samples.detach().squeeze().to(\"cpu\").numpy(), rate=24000))\n",
        "  # Clean up to save RAM\n",
        "  del my_samples,samples\n",
        "\n",
        "speak_voice()\n",
        "FastLanguageModel.for_training(model)"
      ],
      "metadata": {
        "id": "jnQwuFjbubEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "かなりぎこちなく、モデルが全文を発音できてないですね。  \n",
        "It's quite awkward and the model can't pronounce the whole sentence.  \n",
        "\n",
        "それではLoRAを使って微調整を行いましょう  \n",
        "LoRAは全パラメータの 1 ～ 10% のみを更新するので非常に効率的です  \n",
        "Now let's do some fine tuning using LoRA.  \n",
        "LoRA is very efficient as it only updates 1-10% of all parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-22T00:51:37.365079Z",
          "iopub.status.busy": "2025-03-22T00:51:37.364731Z",
          "iopub.status.idle": "2025-03-22T00:51:44.221612Z",
          "shell.execute_reply": "2025-03-22T00:51:44.220949Z",
          "shell.execute_reply.started": "2025-03-22T00:51:37.365045Z"
        },
        "id": "6bZsfBuZDeCL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 64, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 64,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### データ準備 Data Prep  \n",
        "\n",
        "TTSモデルの学習用に設計されたMrDragonFox/Eliseを使用します。データセットが必須の形式（単一話者モデルの場合はテキスト、音声、複数話者モデルの場合はソース、テキスト、音声）に従っていることを確認してください。このセクションは独自のデータセットに合わせて変更できますが、最適な学習には正しい構造を維持することが不可欠です。  \n",
        "\n",
        "We will use the `MrDragonFox/Elise`, which is designed for training TTS models. Ensure that your dataset follows the required format: **text, audio** for single-speaker models or **source, text, audio** for multi-speaker models. You can modify this section to accommodate your own dataset, but maintaining the correct structure is essential for optimal training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-22T00:51:44.222880Z",
          "iopub.status.busy": "2025-03-22T00:51:44.222617Z",
          "iopub.status.idle": "2025-03-22T00:52:16.516878Z",
          "shell.execute_reply": "2025-03-22T00:52:16.516033Z",
          "shell.execute_reply.started": "2025-03-22T00:51:44.222848Z"
        },
        "id": "LjY75GoYUCB8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"MrDragonFox/Elise\", split = \"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-22T00:52:16.518175Z",
          "iopub.status.busy": "2025-03-22T00:52:16.517841Z",
          "iopub.status.idle": "2025-03-22T00:52:35.039329Z",
          "shell.execute_reply": "2025-03-22T00:52:35.038356Z",
          "shell.execute_reply.started": "2025-03-22T00:52:16.518146Z"
        },
        "id": "zK94B-Pfioto",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#@title Tokenization Function\n",
        "\n",
        "import locale\n",
        "import torchaudio.transforms as T\n",
        "import os\n",
        "import torch\n",
        "\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "ds_sample_rate = dataset[0][\"audio\"][\"sampling_rate\"]\n",
        "\n",
        "\n",
        "snac_model = snac_model.to(\"cuda\")\n",
        "def tokenise_audio(waveform):\n",
        "  waveform = torch.from_numpy(waveform).unsqueeze(0)\n",
        "  waveform = waveform.to(dtype=torch.float32)\n",
        "  resample_transform = T.Resample(orig_freq=ds_sample_rate, new_freq=24000)\n",
        "  waveform = resample_transform(waveform)\n",
        "\n",
        "  waveform = waveform.unsqueeze(0).to(\"cuda\")\n",
        "\n",
        "  #generate the codes from snac\n",
        "  with torch.inference_mode():\n",
        "    codes = snac_model.encode(waveform)\n",
        "\n",
        "  all_codes = []\n",
        "  for i in range(codes[0].shape[1]):\n",
        "    all_codes.append(codes[0][0][i].item()+128266)\n",
        "    all_codes.append(codes[1][0][2*i].item()+128266+4096)\n",
        "    all_codes.append(codes[2][0][4*i].item()+128266+(2*4096))\n",
        "    all_codes.append(codes[2][0][(4*i)+1].item()+128266+(3*4096))\n",
        "    all_codes.append(codes[1][0][(2*i)+1].item()+128266+(4*4096))\n",
        "    all_codes.append(codes[2][0][(4*i)+2].item()+128266+(5*4096))\n",
        "    all_codes.append(codes[2][0][(4*i)+3].item()+128266+(6*4096))\n",
        "\n",
        "\n",
        "  return all_codes\n",
        "\n",
        "def add_codes(example):\n",
        "    # Always initialize codes_list to None\n",
        "    codes_list = None\n",
        "\n",
        "    try:\n",
        "        answer_audio = example.get(\"audio\")\n",
        "        # If there's a valid audio array, tokenise it\n",
        "        if answer_audio and \"array\" in answer_audio:\n",
        "            audio_array = answer_audio[\"array\"]\n",
        "            codes_list = tokenise_audio(audio_array)\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping row due to error: {e}\")\n",
        "        # Keep codes_list as None if we fail\n",
        "    example[\"codes_list\"] = codes_list\n",
        "\n",
        "    return example\n",
        "\n",
        "dataset = dataset.map(add_codes, remove_columns=[\"audio\"])\n",
        "\n",
        "tokeniser_length = 128256\n",
        "start_of_text = 128000\n",
        "end_of_text = 128009\n",
        "\n",
        "start_of_speech = tokeniser_length + 1\n",
        "end_of_speech = tokeniser_length + 2\n",
        "\n",
        "start_of_human = tokeniser_length + 3\n",
        "end_of_human = tokeniser_length + 4\n",
        "\n",
        "start_of_ai = tokeniser_length + 5\n",
        "end_of_ai =  tokeniser_length + 6\n",
        "pad_token = tokeniser_length + 7\n",
        "\n",
        "audio_tokens_start = tokeniser_length + 10\n",
        "\n",
        "dataset = dataset.filter(lambda x: x[\"codes_list\"] is not None)\n",
        "dataset = dataset.filter(lambda x: len(x[\"codes_list\"]) > 0)\n",
        "\n",
        "def remove_duplicate_frames(example):\n",
        "    vals = example[\"codes_list\"]\n",
        "    if len(vals) % 7 != 0:\n",
        "        raise ValueError(\"Input list length must be divisible by 7\")\n",
        "\n",
        "    result = vals[:7]\n",
        "\n",
        "    removed_frames = 0\n",
        "\n",
        "    for i in range(7, len(vals), 7):\n",
        "        current_first = vals[i]\n",
        "        previous_first = result[-7]\n",
        "\n",
        "        if current_first != previous_first:\n",
        "            result.extend(vals[i:i+7])\n",
        "        else:\n",
        "            removed_frames += 1\n",
        "\n",
        "    example[\"codes_list\"] = result\n",
        "\n",
        "    return example\n",
        "\n",
        "dataset = dataset.map(remove_duplicate_frames)\n",
        "\n",
        "tok_info = '''*** HERE you can modify the text prompt\n",
        "If you are training a multi-speaker model (e.g., canopylabs/orpheus-3b-0.1-ft),\n",
        "ensure that the dataset includes a \"source\" field and format the input accordingly:\n",
        "- Single-speaker: f\"{example['text']}\"\n",
        "- Multi-speaker: f\"{example['source']}: {example['text']}\"\n",
        "'''\n",
        "print(tok_info)\n",
        "\n",
        "def create_input_ids(example):\n",
        "    # Determine whether to include the source field\n",
        "    #text_prompt = f\"{example['source']}: {example['text']}\" if \"source\" in example else example[\"text\"]\n",
        "    text_prompt = f\"Elise: {example['text']}\"\n",
        "\n",
        "    text_ids = tokenizer.encode(text_prompt, add_special_tokens=True)\n",
        "    text_ids.append(end_of_text)\n",
        "\n",
        "    example[\"text_tokens\"] = text_ids\n",
        "    input_ids = (\n",
        "        [start_of_human]\n",
        "        + example[\"text_tokens\"]\n",
        "        + [end_of_human]\n",
        "        + [start_of_ai]\n",
        "        + [start_of_speech]\n",
        "        + example[\"codes_list\"]\n",
        "        + [end_of_speech]\n",
        "        + [end_of_ai]\n",
        "    )\n",
        "    example[\"input_ids\"] = input_ids\n",
        "    example[\"labels\"] = input_ids\n",
        "    example[\"attention_mask\"] = [1] * len(input_ids)\n",
        "\n",
        "    return example\n",
        "\n",
        "\n",
        "dataset = dataset.map(create_input_ids, remove_columns=[\"text\", \"codes_list\"])\n",
        "columns_to_keep = [\"input_ids\", \"labels\", \"attention_mask\"]\n",
        "columns_to_remove = [col for col in dataset.column_names if col not in columns_to_keep]\n",
        "\n",
        "dataset = dataset.remove_columns(columns_to_remove)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### モデルの学習 / Train the model\n",
        "\n",
        "それでは、Huggingface の `Trainer` を使ってみましょう！ 詳しいドキュメントは [Transformers のドキュメント](https://huggingface.co/docs/transformers/main_classes/trainer) をご覧ください。処理速度を上げるために60ステップ実行しますが、フル実行する場合は `num_train_epochs=1` を設定し、`max_steps=None` をオフにすることもできます。  \n",
        "\n",
        "**注:** per_device_train_batch_size を1より大きい値に設定すると、マルチGPU構成でエラーが発生する可能性があります。問題を回避するには、CUDA_VISIBLE_DEVICES が単一のGPUに設定されていることを確認してください（例: CUDA_VISIBLE_DEVICES=0）。  \n",
        "\n",
        "\n",
        "Now let's use Huggingface  `Trainer`! More docs here: [Transformers docs](https://huggingface.co/docs/transformers/main_classes/trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`.\n",
        "\n",
        "**Note:** Using a per_device_train_batch_size >1 may lead to errors if multi-GPU setup to avoid issues, ensure CUDA_VISIBLE_DEVICES is set to a single GPU (e.g., CUDA_VISIBLE_DEVICES=0)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-22T00:34:09.688959Z",
          "iopub.status.busy": "2025-03-22T00:34:09.688649Z",
          "iopub.status.idle": "2025-03-22T00:34:09.729661Z",
          "shell.execute_reply": "2025-03-22T00:34:09.729001Z",
          "shell.execute_reply.started": "2025-03-22T00:34:09.688939Z"
        },
        "id": "95_Nn-89DhsL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments,Trainer,DataCollatorForSeq2Seq\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    train_dataset = dataset,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 1,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2ejIt2xSNKKp"
      },
      "outputs": [],
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-22T00:34:12.049152Z",
          "iopub.status.busy": "2025-03-22T00:34:12.048862Z",
          "iopub.status.idle": "2025-03-22T00:34:14.404349Z",
          "shell.execute_reply": "2025-03-22T00:34:14.403239Z",
          "shell.execute_reply.started": "2025-03-22T00:34:12.049130Z"
        },
        "id": "yqxqAZ7KJ4oL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pCqnaKmlO1U9"
      },
      "outputs": [],
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### 推論 Inference\n",
        "\n",
        "モデルを動かしみましょう！promptsは自由に変更することができます  \n",
        "Let's run the model! You can change the prompts  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apUdB40Ep6Ki"
      },
      "outputs": [],
      "source": [
        "prompts = [\n",
        "    \"Hey there my name is Elise, <giggles> and I'm a speech generation model that can sound like a person.\",\n",
        "]\n",
        "\n",
        "chosen_voice = \"Elise\" # None for single-speaker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-22T00:52:35.040842Z",
          "iopub.status.busy": "2025-03-22T00:52:35.040125Z",
          "iopub.status.idle": "2025-03-22T00:52:35.050560Z",
          "shell.execute_reply": "2025-03-22T00:52:35.049663Z",
          "shell.execute_reply.started": "2025-03-22T00:52:35.040818Z"
        },
        "id": "krYI8PrRJ6MX",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#@title Run Inference\n",
        "\n",
        "\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "# Moving snac_model cuda to cpu\n",
        "snac_model.to(\"cpu\")\n",
        "\n",
        "prompts_ = [(f\"{chosen_voice}: \" + p) if chosen_voice else p for p in prompts]\n",
        "\n",
        "speak_voice()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "オリジナルのwebbigdata/VoiceCoreに英語を話させると、非常にぎこちない発音になりますが、わずか60ステップでもFinetuneを適用すると、元のモデルよりもかなり流暢になった事がわかります。  \n",
        "\n",
        "&lt;giggles&gt;タグは笑い声に変換できていないかもしれませんが、パラメータとデータセットを調整することでより品質を向上させることができます。  \n",
        "\n",
        "If you make the original webbigdata/VoiceCore speak English, it will have a very awkward pronunciation, but if you apply Finetune even for just 60 steps, you can see that it is much more fluent than the original model.\n",
        "\n",
        "The &lt;giggles&gt; tag may not be converted to laughter, but you can improve the quality by adjusting the parameters and dataset."
      ],
      "metadata": {
        "id": "oTGlEEvBqgHC"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30919,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}